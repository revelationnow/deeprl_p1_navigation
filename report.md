# Navigation Using Deep Reinforcement learning
## Basic Idea
The work for this project relies heavily on the classroom projectfor lunar lander, using 2 hidden layers in the ANN used for estimating the Q values. The process is broken down into two parts, the first is where experience is built over the period of an episode, and the replay buffer is populated with experiences from the actions taken from the initial arbitrary outputs of the ANN. At the end of the episode a learn step is performed, where the Bellman equations are evaluated for the experience that was gained during the episode and saved in the experience replay buffer and the TD error is optimized to get the update values for the weights. As explained during the class, the random access of the Replay buffer helps alleviate issues related to temporal correlation of actions, and the use of two networks, one a target network and another a principal or local network allows for reduction in artifacts that might arise due to the weight update on the network affecting the Q values, causing some kind of oscillations or divergence, as explained in the donkey and carrot example. The weights of the target network are updated slowly, but used in equation for the TD error as a proxy for the Q value of the ideal policy q_pi, and they are updated at the end of each episode.
The differences from the Lunar lander project are primarily limited to the updates to the number of states and the way the environment steps and resets are handled in the code.
## Further Work
Currently the ANN used is a simple network with 2 fully connected hidden layers. This could be experimented with, possibly leading to networks which might converge faster. Additionally other mechanisms could be employed such as Double DQN, Prioritized Experience Replay, Dueling Q Network etc. Additionally the network currently relies on the 37 dimensional state space that it receives from the Unity environment to decide on actions, a more challenging problem would be what is given in the optional exercise, which would be to run with the state being the pixel values of the image that is seen on the screen.
